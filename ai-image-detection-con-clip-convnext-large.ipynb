{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\n\n⚠️ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) 🙏","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"# ==========================================\n# AI IMAGE DETECTION CON GRADIO SU KAGGLE\n# ==========================================\n\n# 1. INSTALLAZIONE DELLE DIPENDENZE\nimport subprocess\nimport sys\n\ndef install_packages():\n    \"\"\"Installa i pacchetti necessari su Kaggle\"\"\"\n    packages = [\n        'gradio',\n        'open-clip-torch',\n        'timm',  # Richiesto da open-clip\n        'ftfy',  # Per il text processing\n        'regex',  # Per il tokenizer\n        'plotly',  # Per grafici interattivi\n        'kaleido'  # Per esportare grafici plotly\n    ]\n    \n    for package in packages:\n        try:\n            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n            print(f\"✓ {package} installato con successo\")\n        except subprocess.CalledProcessError:\n            print(f\"✗ Errore nell'installazione di {package}\")\n\n# Esegui installazione\ninstall_packages()\n\n# 2. IMPORT E CONFIGURAZIONE\nimport open_clip\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport gradio as gr\nfrom pathlib import Path\nimport json\nimport warnings\nimport io\nimport base64\nimport time\nwarnings.filterwarnings('ignore')\n\n# Configurazione dispositivo\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Dispositivo utilizzato: {device}\")\n\n# 3. CARICAMENTO DEL MODELLO\ndef load_clip_model():\n    \"\"\"Carica il modello CLIP ottimizzato per Kaggle\"\"\"\n    try:\n        model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n            'hf-hub:laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup'\n        )\n        tokenizer = open_clip.get_tokenizer('hf-hub:laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\n        \n        model = model.to(device)\n        model.eval()\n        \n        print(\"✓ Modello CLIP caricato con successo\")\n        return model, preprocess_val, tokenizer\n    except Exception as e:\n        print(f\"✗ Errore nel caricamento del modello: {e}\")\n        print(\"Tentativo con modello alternativo...\")\n        try:\n            model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n                'ViT-B-32', pretrained='openai'\n            )\n            tokenizer = open_clip.get_tokenizer('ViT-B-32')\n            model = model.to(device)\n            model.eval()\n            print(\"✓ Modello alternativo caricato con successo\")\n            return model, preprocess_val, tokenizer\n        except Exception as e2:\n            print(f\"✗ Errore anche con modello alternativo: {e2}\")\n            return None, None, None\n\n# Carica il modello\nmodel, preprocess_val, tokenizer = load_clip_model()\n\n# 4. CLASSE PER AI DETECTION\nclass AIImageDetector:\n    def __init__(self, model, preprocess_val, tokenizer, device):\n        self.model = model\n        self.preprocess_val = preprocess_val\n        self.tokenizer = tokenizer\n        self.device = device\n        \n        # Prompt ottimizzati per diversi scenari\n        self.detection_prompts = {\n            'real_photo': [\n                \"a natural photograph with authentic lighting and realistic textures\",\n                \"a real photo taken with a camera showing natural imperfections\",\n                \"an authentic photograph with natural grain and lighting\",\n                \"a genuine photo with realistic shadows and highlights\",\n                \"a natural image with organic textures and realistic details\"\n            ],\n            'ai_generated': [\n                \"an AI-generated image with synthetic artifacts and artificial patterns\",\n                \"a digitally generated image created by artificial intelligence\",\n                \"a synthetic image with overly smooth textures and perfect details\",\n                \"an artificial image with computer-generated characteristics\",\n                \"a machine-generated image with typical AI artifacts\"\n            ],\n            'stable_diffusion': [\n                \"image generated by Stable Diffusion with characteristic noise patterns\",\n                \"AI artwork with typical diffusion model artifacts and smooth textures\",\n                \"Stable Diffusion generated image with distinctive rendering style\"\n            ],\n            'midjourney': [\n                \"Midjourney generated image with distinctive artistic style\",\n                \"AI art with Midjourney's characteristic color palette and composition\",\n                \"Midjourney artwork with typical stylistic elements\"\n            ],\n            'dalle': [\n                \"DALL-E generated image with typical OpenAI model characteristics\",\n                \"AI image with DALL-E style rendering and texture patterns\",\n                \"DALL-E artwork with characteristic generation artifacts\"\n            ]\n        }\n    \n    def detect_single_image(self, image_input, detailed=True):\n        \"\"\"\n        Rileva se una singola immagine è generata da AI\n        \"\"\"\n        try:\n            # Gestisce input diversi (path, PIL Image, numpy array)\n            if isinstance(image_input, str):\n                image = Image.open(image_input).convert('RGB')\n            elif isinstance(image_input, np.ndarray):\n                image = Image.fromarray(image_input).convert('RGB')\n            else:\n                image = image_input.convert('RGB')\n            \n            image_tensor = self.preprocess_val(image).unsqueeze(0).to(self.device)\n            \n            if detailed:\n                return self._detailed_detection(image_tensor)\n            else:\n                return self._simple_detection(image_tensor)\n                \n        except Exception as e:\n            return {'error': str(e)}\n    \n    def _simple_detection(self, image_tensor):\n        \"\"\"Rilevamento semplice e veloce\"\"\"\n        prompts = [\n            \"a natural photograph with authentic lighting\",\n            \"an AI-generated image with synthetic artifacts\"\n        ]\n        \n        text_tokens = self.tokenizer(prompts).to(self.device)\n        \n        with torch.no_grad():\n            image_features = self.model.encode_image(image_tensor)\n            text_features = self.model.encode_text(text_tokens)\n            \n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n            \n            similarities = (image_features @ text_features.T).softmax(dim=-1)[0]\n        \n        ai_probability = float(similarities[1])\n        \n        return {\n            'ai_probability': ai_probability,\n            'classification': 'AI-generated' if ai_probability > 0.5 else 'Real',\n            'confidence': float(abs(similarities[1] - similarities[0]))\n        }\n    \n    def _detailed_detection(self, image_tensor):\n        \"\"\"Rilevamento dettagliato con analisi per tipo di generatore\"\"\"\n        all_prompts = []\n        prompt_labels = []\n        \n        for category, prompts in self.detection_prompts.items():\n            all_prompts.extend(prompts)\n            prompt_labels.extend([category] * len(prompts))\n        \n        text_tokens = self.tokenizer(all_prompts).to(self.device)\n        \n        with torch.no_grad():\n            image_features = self.model.encode_image(image_tensor)\n            text_features = self.model.encode_text(text_tokens)\n            \n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n            \n            similarities = (image_features @ text_features.T).softmax(dim=-1)[0]\n        \n        # Raggruppa i risultati per categoria\n        category_scores = {}\n        for i, (score, label) in enumerate(zip(similarities, prompt_labels)):\n            if label not in category_scores:\n                category_scores[label] = []\n            category_scores[label].append(float(score))\n        \n        # Calcola medie per categoria\n        category_averages = {k: np.mean(v) for k, v in category_scores.items()}\n        \n        # Determina la classificazione\n        max_category = max(category_averages, key=category_averages.get)\n        is_ai = max_category != 'real_photo'\n        \n        return {\n            'is_ai_generated': is_ai,\n            'most_likely_source': max_category,\n            'category_scores': category_averages,\n            'confidence': float(category_averages[max_category]),\n            'ai_probability': 1 - category_averages.get('real_photo', 0),\n            'suspected_generator': max_category if is_ai else None\n        }\n    \n    def batch_detect(self, image_paths, detailed=False):\n        \"\"\"\n        Analizza multiple immagini in batch\n        \"\"\"\n        results = []\n        \n        for i, image_path in enumerate(image_paths):\n            print(f\"Analizzando immagine {i+1}/{len(image_paths)}: {image_path}\")\n            \n            result = self.detect_single_image(image_path, detailed=detailed)\n            result['image_path'] = str(image_path)\n            results.append(result)\n        \n        return results\n\n# 5. FUNZIONI DI VISUALIZZAZIONE CON PLOTLY\ndef create_interactive_charts(results):\n    \"\"\"\n    Crea grafici interattivi con Plotly\n    \"\"\"\n    if isinstance(results, dict):\n        results = [results]\n    \n    # Converte in DataFrame se necessario\n    if not isinstance(results, pd.DataFrame):\n        df = pd.DataFrame(results)\n    else:\n        df = results\n    \n    # Grafico a torta per classificazione\n    classification_counts = df['classification'].value_counts() if 'classification' in df.columns else pd.Series(['Real', 'AI-generated'], index=[0, 1])\n    \n    fig_pie = px.pie(\n        values=classification_counts.values,\n        names=classification_counts.index,\n        title=\"Distribuzione Classificazioni\",\n        color_discrete_map={'Real': 'green', 'AI-generated': 'red'}\n    )\n    \n    # Grafico a barre per probabilità AI\n    if len(df) > 1:\n        fig_bar = px.histogram(\n            df, \n            x='ai_probability', \n            title='Distribuzione Probabilità AI',\n            nbins=20,\n            labels={'ai_probability': 'Probabilità AI', 'count': 'Frequenza'}\n        )\n    else:\n        fig_bar = go.Figure()\n        fig_bar.add_trace(go.Bar(x=['AI Probability'], y=[df['ai_probability'].iloc[0]]))\n        fig_bar.update_layout(title='Probabilità AI')\n    \n    return fig_pie, fig_bar\n\ndef create_detailed_analysis_chart(result):\n    \"\"\"\n    Crea un grafico dettagliato per singola immagine\n    \"\"\"\n    if 'category_scores' in result:\n        categories = list(result['category_scores'].keys())\n        scores = list(result['category_scores'].values())\n        \n        fig = px.bar(\n            x=categories,\n            y=scores,\n            title=\"Punteggi per Categoria\",\n            labels={'x': 'Categoria', 'y': 'Punteggio'},\n            color=scores,\n            color_continuous_scale='RdYlBu_r'\n        )\n        \n        fig.update_layout(showlegend=False)\n        return fig\n    \n    return None\n\n# 6. INTERFACCIA GRADIO\ndef create_gradio_interface():\n    \"\"\"\n    Crea l'interfaccia Gradio per l'AI detection\n    \"\"\"\n    if model is None:\n        return gr.Interface(\n            fn=lambda x: \"Errore: Modello non caricato\",\n            inputs=gr.Image(type=\"pil\"),\n            outputs=gr.Textbox(),\n            title=\"AI Image Detection - Errore\"\n        )\n    \n    detector = AIImageDetector(model, preprocess_val, tokenizer, device)\n    \n    def analyze_image(image, analysis_type):\n        \"\"\"\n        Funzione principale per analizzare un'immagine\n        \"\"\"\n        if image is None:\n            return \"Carica un'immagine per iniziare l'analisi\", None, None, None\n        \n        start_time = time.time()\n        detailed = analysis_type == \"Dettagliata\"\n        \n        result = detector.detect_single_image(image, detailed=detailed)\n        \n        if 'error' in result:\n            return f\"Errore nell'analisi: {result['error']}\", None, None, None\n        \n        analysis_time = time.time() - start_time\n        \n        # Crea il testo di output\n        output_text = f\"\"\"\n## 🔍 Risultati dell'Analisi\n\n**Classificazione:** {'🤖 AI-Generated' if result.get('is_ai_generated', result.get('classification') == 'AI-generated') else '📷 Real Photo'}\n\n**Probabilità AI:** {result['ai_probability']:.1%}\n\n**Confidence:** {result['confidence']:.1%}\n\n**Tempo di analisi:** {analysis_time:.2f} secondi\n\n---\n\"\"\"\n        \n        if detailed and 'category_scores' in result:\n            output_text += \"\\n### 📊 Analisi Dettagliata per Categoria:\\n\"\n            for category, score in result['category_scores'].items():\n                emoji = \"🎨\" if \"ai\" in category or category in [\"stable_diffusion\", \"midjourney\", \"dalle\"] else \"📷\"\n                output_text += f\"**{emoji} {category.replace('_', ' ').title()}:** {score:.1%}\\n\"\n            \n            if result['suspected_generator'] and result['suspected_generator'] != 'real_photo':\n                output_text += f\"\\n**🎯 Generatore Sospetto:** {result['suspected_generator'].replace('_', ' ').title()}\"\n        \n        # Crea grafici\n        fig_pie, fig_bar = create_interactive_charts(result)\n        detailed_chart = create_detailed_analysis_chart(result) if detailed else None\n        \n        return output_text, fig_pie, fig_bar, detailed_chart\n    \n    def analyze_batch(files, analysis_type):\n        \"\"\"\n        Analizza multiple immagini\n        \"\"\"\n        if not files:\n            return \"Carica almeno un'immagine\", None, None, None\n        \n        detailed = analysis_type == \"Dettagliata\"\n        results = []\n        \n        for file in files:\n            try:\n                image = Image.open(file.name)\n                result = detector.detect_single_image(image, detailed=detailed)\n                result['filename'] = file.name\n                results.append(result)\n            except Exception as e:\n                results.append({'filename': file.name, 'error': str(e)})\n        \n        # Crea report\n        total_images = len(results)\n        ai_images = sum(1 for r in results if r.get('is_ai_generated', False) or r.get('classification') == 'AI-generated')\n        real_images = total_images - ai_images\n        \n        output_text = f\"\"\"\n## 📊 Analisi Batch Completata\n\n**Totale immagini:** {total_images}\n**Immagini AI:** {ai_images} ({ai_images/total_images:.1%})\n**Immagini reali:** {real_images} ({real_images/total_images:.1%})\n\n---\n\n### 📋 Risultati per immagine:\n\"\"\"\n        \n        for result in results:\n            if 'error' in result:\n                output_text += f\"❌ **{result['filename']}:** Errore - {result['error']}\\n\"\n            else:\n                classification = '🤖 AI' if result.get('is_ai_generated', result.get('classification') == 'AI-generated') else '📷 Real'\n                output_text += f\"{classification} **{result['filename']}:** {result['ai_probability']:.1%} probabilità AI\\n\"\n        \n        # Crea grafici per il batch\n        df = pd.DataFrame([r for r in results if 'error' not in r])\n        if len(df) > 0:\n            fig_pie, fig_bar = create_interactive_charts(df)\n            return output_text, fig_pie, fig_bar, None\n        else:\n            return output_text, None, None, None\n    \n    # Interfaccia per singola immagine\n    with gr.Blocks(title=\"AI Image Detection\", theme=gr.themes.Soft()) as single_interface:\n        gr.Markdown(\"\"\"\n        # 🔍 AI Image Detection\n        ### Rileva se un'immagine è generata artificialmente o è una foto reale\n        \"\"\")\n        \n        with gr.Row():\n            with gr.Column(scale=1):\n                image_input = gr.Image(\n                    type=\"pil\",\n                    label=\"Carica un'immagine\",\n                    height=400\n                )\n                \n                analysis_type = gr.Radio(\n                    choices=[\"Semplice\", \"Dettagliata\"],\n                    value=\"Dettagliata\",\n                    label=\"Tipo di analisi\"\n                )\n                \n                analyze_btn = gr.Button(\"🔍 Analizza Immagine\", variant=\"primary\")\n                \n                gr.Markdown(\"\"\"\n                ### 📖 Come funziona:\n                - **Semplice**: Veloce, determina solo se è AI o reale\n                - **Dettagliata**: Identifica il possibile generatore AI (Stable Diffusion, Midjourney, DALL-E)\n                \"\"\")\n            \n            with gr.Column(scale=2):\n                output_text = gr.Markdown(label=\"Risultati\")\n                \n                with gr.Row():\n                    pie_chart = gr.Plot(label=\"Classificazione\")\n                    bar_chart = gr.Plot(label=\"Probabilità\")\n                \n                detailed_chart = gr.Plot(label=\"Analisi Dettagliata\", visible=False)\n        print(output_text)\n        analyze_btn.click(\n            fn=analyze_image,\n            inputs=[image_input, analysis_type],\n            outputs=[output_text, pie_chart, bar_chart, detailed_chart]\n        )\n        \n        # Mostra grafico dettagliato solo per analisi dettagliata\n        analysis_type.change(\n            fn=lambda x: gr.Plot(visible=(x == \"Dettagliata\")),\n            inputs=analysis_type,\n            outputs=detailed_chart\n        )\n    \n    # Interfaccia per batch\n    with gr.Blocks(title=\"AI Image Detection - Batch\", theme=gr.themes.Soft()) as batch_interface:\n        gr.Markdown(\"\"\"\n        # 📊 AI Image Detection - Batch Analysis\n        ### Analizza multiple immagini contemporaneamente\n        \"\"\")\n        \n        with gr.Row():\n            with gr.Column(scale=1):\n                files_input = gr.Files(\n                    file_types=[\"image\"],\n                    label=\"Carica immagini multiple\"\n                )\n                \n                batch_analysis_type = gr.Radio(\n                    choices=[\"Semplice\", \"Dettagliata\"],\n                    value=\"Semplice\",\n                    label=\"Tipo di analisi\"\n                )\n                \n                batch_analyze_btn = gr.Button(\"📊 Analizza Batch\", variant=\"primary\")\n            \n            with gr.Column(scale=2):\n                batch_output = gr.Markdown(label=\"Risultati Batch\")\n                \n                with gr.Row():\n                    batch_pie = gr.Plot(label=\"Distribuzione\")\n                    batch_bar = gr.Plot(label=\"Probabilità\")\n        print(batch_output, batch_bar, batch_pie)\n\n        batch_analyze_btn.click(\n            fn=analyze_batch,\n            inputs=[files_input, batch_analysis_type],\n            outputs=[batch_output, batch_pie, batch_bar] \n        )\n    \n    # Combina le interfacce\n    interface = gr.TabbedInterface(\n        [single_interface, batch_interface],\n        [\"🔍 Analisi Singola\", \"📊 Analisi Batch\"],\n        title=\"AI Image Detection\"\n    )\n    \n    return interface\n\n# 7. FUNZIONI UTILITY\ndef save_results_to_csv(results, filename=\"/kaggle/working/ai_detection_results.csv\"):\n    \"\"\"\n    Salva i risultati in CSV\n    \"\"\"\n    if isinstance(results, dict):\n        results = [results]\n    \n    df = pd.DataFrame(results)\n    df.to_csv(filename, index=False)\n    print(f\"Risultati salvati in {filename}\")\n    return df\n\ndef create_analysis_report(results, report_path=\"/kaggle/working/analysis_report.json\"):\n    \"\"\"\n    Crea un report dettagliato dell'analisi\n    \"\"\"\n    if isinstance(results, dict):\n        results = [results]\n    \n    df = pd.DataFrame(results)\n    \n    report = {\n        'total_images': len(df),\n        'ai_generated_count': int(df['is_ai_generated'].sum()) if 'is_ai_generated' in df.columns else 0,\n        'real_images_count': int((~df['is_ai_generated']).sum()) if 'is_ai_generated' in df.columns else 0,\n        'average_ai_probability': float(df['ai_probability'].mean()) if 'ai_probability' in df.columns else 0,\n        'average_confidence': float(df['confidence'].mean()) if 'confidence' in df.columns else 0,\n        'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n    }\n    \n    with open(report_path, 'w') as f:\n        json.dump(report, f, indent=2)\n    \n    print(f\"Report salvato in {report_path}\")\n    return report\n\n# 8. AVVIO DELL'APPLICAZIONE\ndef launch_app():\n    \"\"\"\n    Avvia l'applicazione Gradio\n    \"\"\"\n    print(\"🚀 Avvio dell'interfaccia AI Image Detection...\")\n    \n    interface = create_gradio_interface()\n    \n    # Configurazione per Kaggle\n    interface.launch(\n        server_name=\"0.0.0.0\",  # Permette accesso esterno\n        server_port=7860,       # Porta standard\n        share=True,             # Crea link pubblico\n        debug=False,\n        show_error=True,\n        quiet=False\n    )\n\n# 9. ESECUZIONE PRINCIPALE\nif __name__ == \"__main__\":\n    print(\"=== AI IMAGE DETECTION CON GRADIO ===\")\n    \n    if model is not None:\n        print(\"✅ Modello caricato con successo!\")\n        print(\"🌐 Avvio interfaccia web...\")\n        launch_app()\n    else:\n        print(\"❌ Errore nel caricamento del modello\")\n        print(\"Verifica le dipendenze e riprova\")\n        \n        # Interfaccia di errore\n        error_interface = gr.Interface(\n            fn=lambda: \"❌ Modello non disponibile. Verifica l'installazione delle dipendenze.\",\n            inputs=gr.Textbox(placeholder=\"Modello non caricato\"),\n            outputs=gr.Textbox(),\n            title=\"AI Image Detection - Errore\"\n        )\n        error_interface.launch(share=True)\n\n# 10. FUNZIONI AGGIUNTIVE PER KAGGLE\ndef quick_test():\n    \"\"\"\n    Test rapido dell'interfaccia\n    \"\"\"\n    if model is None:\n        print(\"Modello non disponibile per il test\")\n        return\n    \n    print(\"Test dell'interfaccia...\")\n    detector = AIImageDetector(model, preprocess_val, tokenizer, device)\n    \n    # Test con immagine di esempio (sostituisci con un'immagine reale)\n    try:\n        # Crea un'immagine di test\n        test_image = Image.new('RGB', (224, 224), color='red')\n        result = detector.detect_single_image(test_image, detailed=True)\n        print(f\"Test completato: {result}\")\n        return result\n    except Exception as e:\n        print(f\"Errore nel test: {e}\")\n        return None\n\n# Test automatico all'avvio\nif model is not None:\n    print(\"\\n🧪 Esecuzione test rapido...\")\n    test_result = quick_test()\n    if test_result:\n        print(\"✅ Test completato con successo!\")\n    else:\n        print(\"⚠️ Test fallito, ma l'interfaccia dovrebbe comunque funzionare\")","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:16:17.467872Z","iopub.execute_input":"2025-07-09T16:16:17.468169Z","iopub.status.idle":"2025-07-09T16:16:47.188129Z","shell.execute_reply.started":"2025-07-09T16:16:17.468148Z","shell.execute_reply":"2025-07-09T16:16:47.187347Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\nRequirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\nRequirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.5.1)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n✓ gradio installato con successo\nRequirement already satisfied: open-clip-torch in /usr/local/lib/python3.11/dist-packages (2.32.0)\nRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.21.0+cu124)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2024.11.6)\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (6.3.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.3)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (1.1.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->open-clip-torch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->open-clip-torch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\n✓ open-clip-torch installato con successo\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\n✓ timm installato con successo\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n✓ ftfy installato con successo\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n✓ regex installato con successo\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (25.0)\n✓ plotly installato con successo\nRequirement already satisfied: kaleido in /usr/local/lib/python3.11/dist-packages (1.0.0)\nRequirement already satisfied: choreographer>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from kaleido) (1.0.9)\nRequirement already satisfied: logistro>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from kaleido) (1.1.0)\nRequirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.11/dist-packages (from kaleido) (3.10.18)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kaleido) (25.0)\nRequirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.11/dist-packages (from choreographer>=1.0.5->kaleido) (3.20.1)\n✓ kaleido installato con successo\nDispositivo utilizzato: cuda\n✓ Modello CLIP caricato con successo\n=== AI IMAGE DETECTION CON GRADIO ===\n✅ Modello caricato con successo!\n🌐 Avvio interfaccia web...\n🚀 Avvio dell'interfaccia AI Image Detection...\n<gradio.components.markdown.Markdown object at 0x781adcc498d0>\n<gradio.components.markdown.Markdown object at 0x781ad88c9c90> <gradio.components.plot.Plot object at 0x781ad87f6890> <gradio.components.plot.Plot object at 0x781ad88e4650>\n* Running on local URL:  http://0.0.0.0:7860\n* Running on public URL: https://1291f1570c49f63c81.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://1291f1570c49f63c81.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"\n🧪 Esecuzione test rapido...\nTest dell'interfaccia...\nTest completato: {'is_ai_generated': True, 'most_likely_source': 'stable_diffusion', 'category_scores': {'real_photo': 0.05114610195159912, 'ai_generated': 0.05375513657927513, 'stable_diffusion': 0.0539827731748422, 'midjourney': 0.05184286584456762, 'dalle': 0.052672321597735085}, 'confidence': 0.0539827731748422, 'ai_probability': 0.9488538980484009, 'suspected_generator': 'stable_diffusion'}\n✅ Test completato con successo!\n","output_type":"stream"},{"name":"stderr","text":"ERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 840, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 856, in simple_response\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/gradio/routes.py\", line 1597, in upload_file\n    form = await multipart_parser.parse()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 694, in parse\n    async for chunk in self.stream:\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/requests.py\", line 235, in stream\n    raise ClientDisconnect()\nstarlette.requests.ClientDisconnect\n","output_type":"stream"}],"execution_count":9}]}